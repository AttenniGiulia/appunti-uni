\input{../common/common.tex}
\renewcommand*\rmdefault{ppl}

\begin{document}

\chapter{Funzioni e limiti}

\section{Funzioni trigonometriche}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
\begin{axis}[
    xticklabels={,,},
    yticklabels={,,},
    ymin=-2,
    ymax=2,
    % enlargelimits=true,
    axis lines=middle,
    % clip=false,
    domain=-7:7,
    axis on top
    ]

\addplot[smooth, dashed, domain=-7:7, green]{1};
\addplot[smooth, dashed, domain=-7:7, green]{-1};
\addplot[smooth, thick,domain=-6.28:6.28,samples=40]{sin(deg(x))};
\node[label={225:{$-1$}},circle,fill,inner sep=2pt] at (axis cs:0,-1) {};
\node[label={45:{$1$}},circle,fill,inner sep=2pt] at (axis cs:0,1) {};
\node[label={225:{$\pi$}},circle,fill,inner sep=2pt] at (axis cs:3.14,0) {};
\node[label={90:{$2\, \pi$}},circle,fill,inner sep=2pt] at (axis cs:6.28,0) {};
\end{axis}
\end{tikzpicture}
\qquad
\begin{tikzpicture}[scale=1.0]
\begin{axis}[
    xticklabels={,,},
    yticklabels={,,},
    ymin=-2,
    ymax=2,
    % enlargelimits=true,
    axis lines=middle,
    % clip=false,
    domain=-7:7,
    axis on top
    ]

\addplot[smooth, dashed, domain=-7:7, green]{1};
\addplot[smooth, dashed, domain=-7:7, green]{-1};
\addplot[smooth, thick,domain=-6.28:6.28,samples=40]{cos(deg(x))};
\node[label={225:{$-1$}},circle,fill,inner sep=2pt] at (axis cs:0,-1) {};
\node[label={45:{$1$}},circle,fill,inner sep=2pt] at (axis cs:0,1) {};
\node[label={270:{$\pi$}},circle,fill,inner sep=2pt] at (axis cs:3.14,0) {};
\node[label={90:{$2\, \pi$}},circle,fill,inner sep=2pt] at (axis cs:6.28,0) {};
\end{axis}
\end{tikzpicture}
\caption{Seno e coseno}
\end{figure}

Seno e coseno oscillano fra $-1$ e 1, quindo $-1 \le \sin(x) \le 1$ e 
$-1 \le \cos(x) \le 1$, ossia $\abs{\sin(x)} \le 1$ e $\abs{\cos(x)} \le 1$.
Di conseguenza vale anche che $\abs{\sin^2(x)} \le 1$ e 
$\abs{\cos^2(x)} \le 1$.
\[
\nexists \lim_{x \to \infty} \sin(x) 
\text{ poich\'e continua a oscillare fra 1 e $-1$}
\]
Questo vale anche per le altre funzioni trigonometriche.

I valori delle altre funzioni trigonometriche si possono ricavare dal
seno e dal coseno:
\begin{align*}
\tan = \frac{\sin}{\cos} &\qquad \cot = \frac{\cos}{\sin} \\
\sec = \frac{1}{\cos} &\qquad \csc = \frac{1}{\sin}
\end{align*}
Un'equivalenza da tenere a mente \`e:
\[
\cos^2 (x) + \sin^2 (x) = 1
\]

Seno e coseno sono funzioni periodiche, di periodo $2 \, \pi$, ossia:
\[
\sin (x) = \sin (x \pm 2 \, k \, \pi) \qquad
\cos (x) = \cos (x \pm 2 \, k \, \pi)
\]

Per i valori di seno, coseno, tangente e cotangente:
\begin{center}
\begin{tabular}{r*{9}{|c}}
gradi & $0^{\circ}$ & $30^{\circ}$ & $45^{\circ}$ & $60^{\circ}$  & $90^{\circ}$ & $120^{\circ}$ & $135^{\circ}$ & $150^{\circ}$ & $180^{\circ}$ \\
\hline
$\sin$ & 0 & $\frac{1}{2}$ & $\frac{\sqrt{2}}{2}$ & $\frac{\sqrt{3}}{2}$ & 1 & $\frac{\sqrt{3}}{2}$ & $\frac{\sqrt{2}}{2}$ & $\frac{1}{2}$ & 0 \\
$\cos$ & 1 & $\frac{\sqrt{3}}{2}$ & $\frac{\sqrt{2}}{2}$ & $\frac{1}{2}$ & 0 & $-\frac{1}{2}$ & $- \frac{\sqrt{2}}{2}$ & $- \frac{\sqrt{3}}{2}$ & $-1$ \\
\hline
$\tan$ & 0 & $\frac{\sqrt{3}}{3}$ & 1 & $\sqrt{3}$ & $\nexists$ & $-\sqrt{3}$ & $-1$ & $-\frac{\sqrt{3}}{3}$ & 0 \\
$\cot$ & $\nexists$ & $\sqrt{3}$ & $1$ & $\frac{\sqrt{3}}{3}$ & 0 & $-\frac{\sqrt{3}}{3}$ & $- 1$ & $- \sqrt{3}$ & $\nexists$ \\
\hline
rad. & 0 & $\frac{\pi}{6}$ & $\frac{\pi}{4}$ & $\frac{\pi}{3}$  & $\frac{\pi}{2}$ & $\frac{2}{3} \, \pi$ & $\frac{3}{4} \, \pi$ & $\frac{5}{6} \, \pi$ & $\pi$ 
\end{tabular}
\end{center}
La tabella aiuta anche a trovare i valori delle funzioni inverse delle
funzioni trigonometriche: per conoscere il valore di $\arccos\left(1\right)$,
\`e sufficiente trovare la colonna della tabella per cui il coseno vale 1 
(ossia, a $0^{\circ}$).

Per trovare i valori di seno e coseno fra $180^{\circ}$ e $360^{\circ}$ 
(ossia fra $\pi$ e $2 \, \pi$), si possono usare queste due uguaglianze:
\begin{align*}
\sin(-x) = - \sin(x) \qquad \cos(-x) = \cos(x)
\end{align*}
Ci dicono che seno e coseno sono rispettivamente una funzione dispari 
e una funzione pari.

Entrambe le funzioni hanno un periodi di $2 \, \pi$, ossia di $360^{\circ}$.
Quindi per trovare il valore di $\sin(330^{\circ})$, basta poco:
\[
\sin(330^{\circ}) = \sin(360^{\circ} - 30^{\circ}) =
\sin(-30^{\circ}) = - \sin(30^{\circ}) = - \frac{1}{2}
\]
La tangente e la cotangente hanno dei punti di discontinuit\`a, in cui 
tendono all'infinito (positivo o negativo). Per sapere a cosa tendono, si
pu\`o consultare il segno del coseno e del seno nella tabella.

Ad esempio, a sinistra di $90^{\circ}$, seno e coseno hanno segni concordi,
quindi la tangente tende all'infinito positivo. A destra di $90^{\circ}$, 
invece, seno e coseno hanno segni discordi, quindi la tangente tende
all'infinito negativo.

\subsection{Duplicazione}

Le formule di ``duplicazione'' che si vedono generalmente:
\begin{align*}
\cos(\alpha + \beta) &= \cos(\alpha) \, \cos(\beta) - \sin(\alpha) \, \sin(\beta) \\
\cos(\alpha - \beta) &= \cos(\alpha) \, \cos(\beta) + \sin(\alpha) \, \sin(\beta) \\
\sin(\alpha + \beta) &= \sin(\alpha) \, \cos(\beta) + \cos(\alpha) \, \sin(\beta)\\
\sin(\alpha - \beta) &= \sin(\alpha) \, \cos(\beta) - \cos(\alpha) \, \sin(\beta)
\end{align*}
% Sono facili da memorizzare: \emph{coscia coscia, seno seno} e \emph{seno coscia, coscia seno}.

Da queste si ricavano due formule di vera duplicazione:
\begin{align*}
\sin( 2 \, x) &= 2 \, \sin (x) \, \cos (x) \\
\cos (2 \, x) &= \cos^2 (x) - \sin^2 (x)
\end{align*}
E dall'ultima in particolare, si tira fuori un'equivalenza per il $\sin^2$ e il $\cos^2$:
\begin{align*}
\cos^2 (x) &= \frac{1 + \cos (2 \, x)}{2} \\
\sin^2 (x) &= \frac{1 - \cos (2 \, x)}{2}
\end{align*}

Un'ultima formula di duplicazione, generale ma poco utile:
\[
\sin(\alpha) - \sin(\beta) = 
2 \, \sin \left( \frac{\alpha - \beta}{2} \right) \, \cos \left( \frac{\alpha + \beta}{2} \right)
\]

\subsection{Limiti notevoli}

\begin{align*}
\lim_{x \to 0} \frac{\sin (x)}{x} = 1
&\qquad
\lim_{x \to 0} \frac{\tan (x)}{x} = 1
\\
\lim_{x \to 0} \frac{1 - \cos(x)}{x^2} = \frac{1}{2}
&\qquad
\lim_{x \to 0} \frac{1 - \cos (x)}{x} = 0
\\
\end{align*}

\chapter{Integrali}

Con il simbolo di \emph{integrale} si indicano tutte le primitive di una funzione $f(x)$:
\[
\int f(x) \dx = F(x) + c
\]
Una primitiva di $f(x)$ verifica la propriet\`a $[F(x)]' = f(x)$, ossia
la sua derivata \`e $f(x)$.
Il simbolo $\diff x$ significa ``rispetto a $x$'', e indica la variabile 
che si sta integrando.

\emph{In un intervallo}, le primitive di una funzione differiscono tutte 
di una costante. Quindi se $F(x)$ \`e una primitiva di $f(x)$, scrivendo
$F(x) + c$ stiamo indicando \emph{tutte} le primitive di $f(x)$ 
(nell'intervallo di integrazione).

\emph{L'integrale \`e l'operazione inversa della derivata.}

\section{Regole di derivazione}

Dalle regole di derivazione si ricavano diverse regole di integrazione 
facilmente applicabili.

La prima regola da ricordare \`e che la derivata \`e \emph{lineare}:
\[
\left[ a \cdot F(x) + b \cdot G(x) \right]' =
a \cdot f(x) + b \cdot g(x)
\]

La derivata di una costante $c$ \`e sempre 0.

La derivata di una potenza di $x$ \`e:
\[
\left[ x^{\alpha} \right]' = \alpha \cdot x^{\alpha - 1}
\]
Chiaramente se $\alpha = 1$, la derivata di $x^{\alpha}$ \`e $x^0 = 1$.

La derivata di un prodotto di funzioni \`e:
\[
\left[ F(x) \cdot G(x) \right]' = 
f(x) \cdot G(x) + F(x) \cdot g(x)
\]
Indichiamo con $f(x)$ la derivata di $F(x)$, e con $g(x)$ la derivata 
di $G(x)$.

La derivata del rapporto di due funzioni \`e:
\[
\left[ \frac{F(x)}{G(x)} \right]' =
\frac{f(x) \cdot G(x) - F(x) \cdot g(x)}{\left(G(x)\right)^2}
\]
Da questa regola si ricava anche una regola per la derivata del
reciproco di una funzione:
\[
\left[ \frac{1}{G(x)} \right]' =
- \frac{g(x)}{\left(G(x)\right)^2}
\]
\`E come se avessimo applicato la regola della derivata del rapporto, 
ponendo $F(x) = 1$, e quindi $f(x) = 0$.

La derivata di una funzione composta \`e:
\[
\left[ F \left(G(x) \right) \right]' =
f \left( G(x) \right) \cdot g(x)
\]
Si legge come ``la derivata della funzione $F(x)$ \emph{calcolata} 
in $G(x)$, per la derivata della funzione $G(x)$''.

\section{Metodo di sostituzione}
La regola di derivazione delle funzioni composte ci dice questo:
\[
\left[ F \left(G(x) \right) \right]' =
f \left( G(x) \right) \cdot g(x)
\]
Quindi, integrando entrambi i membri (e tenendo a mente che l'integrale
\`e l'operazione inversa della derivata):
\[
\int f \left( G(x) \right) \cdot g(x) \dx =
F \left(G(x) \right) + C
\]
Tipicamente si scrive cos\`i:
\begin{align*}
\int f \left( G(x) \right) \cdot g(x) \dx &=
\int \substitute{f (y) \dx[y]}{y = G(x) ,\, \dx[y] = g(x) \dx} = \\
&= \substitute{F(y) + C}{y = G(x)} = \\
&= F \left( G(x) \right) + C
\end{align*}
Bisogna riconoscere una funzione $G(x)$, composta per una funzione $f(x)$,
e la sua derivata $g(x)$. $G(x)$ diventa la nuova variabile di integrazione
$y$, e $g(x) \dx$ diventa la nuova $\diff y$.

\section{Integrali per parti}

La regola di derivazione del prodotto ci dice:
\[
\left[ F(x) \cdot G(x) \right]' = 
f(x) \cdot G(x) + F(x) \cdot g(x)
\]
Possiamo riscriverla cos\`i:
\[
f(x) \cdot G(x) = 
\left[ F(x) \cdot G(x) \right]' - F(x) \cdot g(x)
\]
Integrando entrambi i membri (e tenendo a mente che l'integrale \`e
l'operazione inversa della derivata):
\[
\int \left( f(x) \cdot G(x) \right) \dx = F(x) \cdot G(x) - \int \left( F(x) \cdot g(x) \right) \dx
\]
Per risolvere un integrale per parti, bisogna riconoscere due funzioni:
una funzione $f(x)$ che si sa integrare, e una funzione $G(x)$ che si sa
derivare.

\section{Integrali razionali}

Gli integrali razionali sono integrali del tipo:
\[
\int \frac{p(x)}{q(x)}
\]
dove $p(x)$ e $q(x)$ sono funzioni razionali (ossia, polinomi).

Se il grado di $p(x)$ \`e maggiore o uguale al grado di $q(x)$,
va fatta una divisione fra polinomi. Il grado di una funzione 
razionale \`e l'esponente maggiore delle $x$ nella funzione.

Un esempio di divisione fra polinomi:
\[
\int \frac{x^2}{x - 4} \dx
\]
Il grado al numeratore \`e maggiore del grado al numeratore, quindi
possiamo dividere:
\begin{center}
\begin{tabular}{rrr|r}
$x^2$ & & & $x - 4$ \\ \cline{4-4}
$-x^2$ & $+4 \, x$ & & $x + 4$ \\ \cline{1-2}
$0$ & $4 \, x$ & & \\
& $- 4 \, x$ & $+ 16$ & \\ \cline{2-3}
& $0$ & $16$ & 
\end{tabular}
\end{center}
Il risultato della divisione \`e quindi:
\[
\int \left[ x + 4 + \frac{16}{x-4} \right] \dx 
\]
Nel caso generale, abbiamo una funzione $\frac{p(x)}{q(x)}$, con il grado 
di $p(x)$ maggiore o uguale al grado di $q(x)$. La divisione sar\`a:
\begin{center}
\begin{tabular}{c|r}
$p(x)$ & $q(x)$ \\ \cline{2-2}
$\vdots$ & $h(x)$ \\ \cline{1-1}
$r(x)$ & 
\end{tabular}
\end{center}
E il risultato, quindi:
\[
h(x) + \frac{r(x)}{q(x)}
\]
Ora il grado di $r(x)$ \`e minore del grado di $q(x)$ (grazie all'algebra).

Gli integrali di funzioni razionali sono \emph{sempre} combinazioni 
lineari di polinomi, logaritmi e arcotangenti.

Da qui in poi assumiamo che il grado del polinomio al numeratore \`e 
minore del grado del polinomio al denominatore. Se non \`e cos\`i, torna 
indietro e fai una divisione fra polinomi.

Se il polinomio al denominatore \`e di primo grado, il risultato \`e un 
logaritmo:
\[
\int \frac{\diff x}{a \, x + b} =
\frac{1}{a} \, \abslog{a \, x + b} + C
\]

Se il polinomio al denominatore \`e di secondo grado, avr\`a la forma:
\[
\int \frac{d \, x + e}{a \, x^2 + b \, x + c} \dx
\]
Se il polinomio al numeratore fosse la derivata del polinomio al 
denominatore, l'integrale si risolverebbe per sostituzione. Altrimenti, 
bisogna tirar fuori questa sostituzione, e lasciare da parte un integrale 
con al numeratore un polinomio di grado 0 (che risolviamo poi).

La derivata del denominatore \`e:
\[
2 \, a \, x + b
\]
Con qualche manipolazione algebrica, possiamo ricavare un multiplo di 
questa derivata dal polinomio al numeratore:
\begin{align*}
d \, x + e &= \frac{2 \, a \, d \, x + 2 \, a \, e}{2 \, a} =
\frac{d}{2 \, a} \cdot \left( 2 \, a \, x + \frac{2 \, a \, e}{d} \right) = \\
&= \frac{d}{2 \, a} \cdot \left( 2 \, a \, x + b + \frac{2 \, a \, e - b \, d}{d} \right) = \\
&= \frac{d}{2 \, a} \cdot \left( 2 \, a \, x + b \right) + \frac{2 \, a \, e - b \, d}{2 \, a}
\end{align*}
\`E brutto, ma sono solo numeri, per fortuna.

L'integrale si riscrive come:
\[
\frac{d}{2 \, a} \, \int \frac{2 \, a \, x + b}{a \, x^2 + b \, x + c} \dx +
\frac{2 \, a \, e - b \, d}{2 \, a} \, \int \frac{\diff x}{a \, x^2 + b \, x + c}
\]
Il primo si risolve per sostituzione (\`e un logaritmo), per il secondo,
invece, distinguiamo tre casi in base al determinante del denominatore.
\begin{itemize}
    \item $b^2 > 4 \, a \, c$, quindi il determinante del polinomio \`e 
    positivo. Si trovano le radici $x_1, x_2$. Dopodich\'e, possiamo 
    riscrivere l'integrale come:
    \[
    \int \frac{A}{x - x_1} \dx + \int \frac{B}{x - x_2} \dx 
    \]
    Si impone poi il seguente sistema:
    \[
    A \, x - A \, x_2 + B \, x - B \, x_1 = 1 \implies
    \begin{cases}
    A + B = 0 \\
    A \, x_2 + B \, x_1 + 1 = 0
    \end{cases}
    \]
    Si trovano i valori di $A$ e di $B$, e si risolve quindi l'integrale 
    ottenendo due logaritmi.
    \item $b^2 = 4 \, a \, c$, quindi il determinante del polinomio \`e 
    nullo. Il polinomio ha quindi una radice con doppia molteplicit\`a,
    ossia \`e nella forma $(x - x_1)^2$. Lo riscriviamo come:
    \[
    \int \frac{A}{x - x_1} \dx + \int \frac{B \, x}{(x - x_1)^2} \dx
    \]
    Anche qui si impone un sistema:
    \[
    A \, x - A \, x_1 + B \, x = 1 \implies 
    \begin{cases}
    A + B = 0 \\
    A \, x_1 = 1
    \end{cases}
    \]
    Trovati i valori di $A$ e di $B$, otteniamo un logaritmo per il 
    primo integrale e una funzione razionale per il secondo.
    \item $b^2 < 4 \, a \, c$, quindi il determinante del polinomio \`e 
    negativo. Il polinomio non ha radici reali. Lo vediamo nella prossima 
    sezione.
\end{itemize}

\subsection{Integrali razionali di secondo grado con determinante negativo}

Consideriamo integrali del tipo:
\[
\int \frac{\diff x}{x^2 + b \, x + c}
\]
In cui l'equazione di secondo grado al denominatore non ha soluzioni reali.
Se il coefficiente di $x^2$ non \`e 1, possiamo sempre riscriverlo in questo
modo:
\[
\int \frac{\diff x}{a \, x^2 + b \, x + c} = 
\frac{1}{a} \int \frac{\diff x}{x^2 + \frac{b}{a} \, x + \frac{c}{a}}
\]
Per risolvere l'integrale, riscriviamolo come:
\[
\int \frac{\diff x}{\left( x + \frac{b}{2} \right)^2 + 
\left( c - \frac{b^2}{4} \right)}
\]
Sviluppando il quadrato al denominatore otteniamo lo stesso polinomio:
\[
\left( x + \frac{b}{2} \right)^2 + \left( c - \frac{b^2}{4} \right) =
x^2 + \cancel{2} \, \frac{b}{\cancel{2}} + \cancel{\frac{b^2}{4}} + c - \cancel{\frac{b^2}{4}} = 
x^2 + b + c
\]
Quindi stiamo facendo una sostituzione sensata.

Poniamo $d = c - \frac{b^2}{4}$, per semplicit\`a, e facciamo la 
sostituzione $y = x + \frac{b}{2}$ (e $\diff y = \diff x$).
\[
\int \frac{\diff y}{y^2 + d}
\]
Vogliamo che questo integrale ci dia un'arcotangente. Per ottenerlo, dobbiamo
avere un 1 al posto della $d$. Mettiamo quindi in evidenza in questo modo:
\[
\frac{1}{d} \int \frac{\diff y}{\frac{y^2}{d} + 1}
\]
Ora, sostituiamo $z^2 = \frac{y^2}{d}$, ossia $z = \frac{y}{\sqrt{d}}$, 
e quindi $\diff y = \sqrt{d} \dx[z]$, ottenendo:
\[
\frac{\sqrt{d}}{d} \int \frac{\diff z}{z^2 + 1} =
\frac{1}{\sqrt{d}} \int \frac{\diff z}{z^2 + 1}
\]
La soluzione \`e, quindi:
\begin{align*}
\frac{1}{\sqrt{d}} \, \arctan \left( z \right) + C &=
\frac{1}{\sqrt{d}} \, \arctan \left( \frac{y}{\sqrt{d}} \right) + C = \\
&= \frac{1}{\sqrt{d}} \, \arctan \left( \frac{x + \frac{b}{2}}{\sqrt{d}} \right) + C = \\
&= \frac{1}{\sqrt{c - \frac{b^2}{4}}} \, \arctan \left( \frac{x + \frac{b}{2}}{\sqrt{c - \frac{b^2}{4}}} \right) + C
\end{align*}

\chapter{Integrali definiti}

\section{Integrali impropri}

Un integrale improprio \`e un integrale definito del tipo:
\[
\int_{b}^{a} f(x) \dx
\]
In cui almeno uno dei due punti non fa parte dell'insieme di definizione
della funzione: la funzione pu\`o tendere all'infinito, o il punto pu\`o 
essere infinito. Due esempi sono:
\[
\int_{0}^{2} \frac{1}{\sqrt{x}} \dx \qquad 
\int_{1}^{\infty} \frac{1}{x^3} \dx
\]
Si risolvono in due modi: o direttamente, o per confronto.

Direttamente significa calcolare il limite dell'integrale definito, 
facendo tendere l'estremo problematico al punto di discontinuit\`a (o 
all'infinito):
\[
\int_{b}^{a} f(x) \dx = \lim_{z \to b} \increment{\int f(x) \dx}_{z}^{a}
\]
In questo modo \`e spesso possibile calcolare direttamente il valore.

Risolvere un integrale improprio per confronto, significa stabilire se 
questo converge o meno: se \`e maggiore di un integrale divergente, 
diverge anche questo, mentre se \`e minore di un integrale convergente, 
converge anche questo.

\subsection{Con cosa confronto, quando vado a fare un confronto?}

Nel cercare una funzione con cui confrontare, \`e importante anzitutto 
riconoscere le funzioni ``elementari'' che compongono la funzione che 
si sta studiando: tangenti, seni e coseni, esponenziali, logaritmi, 
potenze. Si pu\`o poi provare a capire fra quali valori \`e compresa
ognuna di queste funzioni, per maggiorarla o minorarla opportumanente, 
o provare a capire se una di queste funzioni \emph{si comporta come} 
qualche altra funzione. Queste due funzioni, ad esempio, sono simili 
quando $x$ tende all'infinito:
\[
\sin \left( \frac{1}{x} \right) \sim \frac{1}{x}
\]
Molti limiti notevoli forniscono esempi di simili funzioni. Il principio
generale \`e il seguente.

Date due funzioni $f(x)$ e $g(x)$, se vale questo:
\[
\lim_{x \to \infty} \frac{f(x)}{g(x)} = l \in \ooint{0}{\infty}
\]
Allora le due funzioni si comportano allo stesso modo. Quindi, se stiamo 
studiando l'integrale improprio da $a$ a $\infty$ di $f(x)$, la convergenza 
di questo integrale dipende dalla convergenza dell'integrale da $a$ a 
$\infty$ dell'integrale di $g(x)$. O convergono entrambe, o divergono 
entrambe.

Nell'intervallo $\ccint{-1}{1}$, vale questo:
\[
x^a \le x^b \iff a \ge b
\]
Se invece $\abs{x} \ge 1$, vale questo:
\[
x^a \le x^b \iff a \le b
\]

Il logaritmo tende all'infinito pi\`u lentamente di ogni potenza positiva:
quindi si pu\`o sempre maggiorare un logaritmo con una potenza $x^a$ con 
$a$ arbitrariamente grande o piccolo.

Vale questo:
\[
f(x) \le g(x) \implies \frac{1}{g(x)} \le \frac{1}{f(x)}
\]

\section{Derivate di integrali definiti}

Consideriamo un integrale di questo tipo:
\[
\int_{A(x)}^{B(x)} g(t) \dx[t]
\]
Indichiamo con $A(x)$ una funzione generica, e con $a(x)$ la sua derivata,
e in generale con una lettera maiuscola una primitiva e con una lettera 
minuscola una derivata.

Sappiamo calcolarne la derivata (grazie al teorema fondamentale del Calcolo),
che \`e:
\[
\left[ \int_{A(x)}^{B(x)} g(t) \dx[t] \right]' =
g \left( B(x) \right) \cdot b(x) - g \left( A(x) \right) \cdot a(x)
\]
Ossia, la derivata dell'integrale dato \`e la funzione integranda calcolata
nella funzione nell'estremo superiore, per la derivata della funzione 
nell'estremo superiore, meno la funzione integranda calcolata nella funzione 
nell'estremo inferiore, per la derivata della funzione nell'estremo 
inferiore.

Pi\`u facile a farsi che a dirsi.
 
Casi pi\`u contorti hanno questa forma:
\[
\left[ F(x) \cdot \int_{A(x)}^{B(x)} g(t) \dx[t] \right]' =
f(x) \cdot \int_{A(x)}^{B(x)} g(t) \dx[t] +
F(x) \cdot \left[ g \left( B(x) \right) \cdot b(x) - g \left( A(x) \right) \cdot a(x) \right]
\]
Notare che il caso precedente \`e una versione particolare di 
quest'ultimo caso, con $F(x) = 1$, e quindi $f(x) = 0$.

\chapter{Equazioni differenziali}

\section{Lineare}

Un'equazione differenziale lineare \`e un'equazione di questo tipo:
\[
y' = a(t) \cdot y
\]
Abbiamo una funzione di $t$ per la $y$. \`E lineare perch\'e la funzione 
della $y$ \`e la retta! Si risolve in questo modo:
\[
\frac{\diff y}{\diff t} = a(t) \cdot y \implies
\frac{\diff y}{y} = a(t) \dx[t]
\]
Integrando entrambi i membri otteniamo:
\[
\int \frac{\diff y}{y} = \int a(t) \dx[t] \implies
\abslog{y} = A(t) + C
\]
Con $A(t)$ indichiamo una primitiva di $a(t)$. Ora, \emph{esponenziamo} 
entrambi i membri:
\[
e^{\abslog{y}} = e^{A(t) + C} \implies
\abs{y} = e^{A(t)} \cdot e^C
\]
$e^C$ \`e una costante positiva, poniamo $K = \pm e^C$, e scriviamo:
\[
y = e^{A(t)} \cdot K
\]

\section{A variabili separabili}

Un'equazione differenziale a variabili separabili \`e un'equazione di 
questo tipo:
\[
y' = a(t) \cdot b(y)
\]
Abbiamo una funzione della $y$ per una funzione della $t$ (o della $x$, 
o comunque non della $y$). \`E a ``variabili separabili'', perch\'e 
possiamo, appunto, separare le variabili, in questo modo:
\[
\frac{\diff y}{\diff t} = a(t) \cdot b(y) \implies
\frac{\diff y}{b(y)} = a(t) \dx[t]
\]
Per risolvere l'equazione si integrano entrambi i membri:
\[
\int \frac{\diff y}{b(y)} = \int a(t) \dx[t]
\]
Da qui... Si \`e da soli. Bisogna risolvere entrambi gli integrali, 
e poi esplicitare la $y$ rispetto alla $t$. Ossia:
\[
B(y) = A(t) + C \implies
y = B^{-1}(A(t) + C)
\]
$B^{-1}$ \`e la funzione inversa di $B$ (ossia, se $B$ \`e il seno, 
$B^{-1}$ \`e l'arcoseno, oppure, se $B$ \`e il logaritmo, $B^{-1}$ \`e 
l'esponenziale).

\section{A variabili non separabili}

Un'equazione non a variabili separabili \`e un'equazione differenziale
di questo tipo:
\[
y' = a(t) \cdot y + b(t)
\]
Abbiamo una funzione $b(t)$, per una funzione (\emph{lineare!}) di $y$, 
pi\`u una funzione di $b(t)$. Non possiamo dividere le funzioni della $y$
e le funzioni della $t$ fra i due membri. La soluzione \`e data da:
\[
y = e^{A(t)} \cdot \left[ \int b(t) \, e^{-A(t)} \dx[t] + C \right]
\]
Dove con $A(t)$ indichiamo una primitiva di $a(t)$. Per risolvere 
l'equazione bisogna quindi trovare questa primitiva, e risolvere 
l'integrale:
\[
\int b(t) \, e^{-A(t)} \dx[t]
\]

\chapter{Serie}

\section{Criteri di convergenza}

Si parla di convergenza assoluta quando vale:
\[
\lim_{N \to \infty} \sum_{n = n_0}^{N} \abs{a_n} = L
\]
Si parla di convergenza semplice quando vale:
\[
\lim_{N \to \infty} \sum_{n = n_0}^{N} a_n = L
\]
Le serie a termini di segno alterno spesso convergono semplicemente,
senza convergere assolutamente. Le serie a termini di segno concorde
se convergono semplicemente convergono anche assolutamente, e viceversa.

Si parla di divergenza quando il limite o non esiste, o tende all'infinito.

\subsection{Criterio del confronto}

Consideriamo due serie $\sum a_n$ e $\sum b_n$, e supponiamo valga questo:
\[
a_n \le b_n \forall n \ge n_0
\]
Vale, di conseguenza, questo:
\[
\sum_{n = n_0}^{\infty} a_n \le \sum_{n = n_0}^{\infty} b_n
\]
Il criterio del confronto ci dice, in questo caso, che:
\begin{itemize}
    \item se $a_n$ diverge, anche $b_n$ diverge;
    \item se $b_n$ converge, anche $a_n$ converge.
\end{itemize}
Se $b_n$ diverge, non sappiamo nulla su $a_n$, e se $a_n$ converge, non 
sappiamo nulla su $b_n$.

\subsection{Criterio integrale}

Se per la serie che si sta studiando vale questo:
\[
f(n) = a_n \forall n > N_0
\]
Allora \`e possibile applicare il criterio integrale. Il criterio integrale
ci dice pressappoco questo:
\[
\sum_{n = N_0 + 1}^{\infty} a_n \le \int_{N_0}^{\infty} f(x) \dx 
\le \sum_{n = N_0}^{\infty} a_n
\]
Ossia vincola la convergenza o divergenza della serie alla convergenza o 
divergenza dell'integrale: se questo converge, la serie converge, e se
questo diverge, anche la serie diverge.

\subsection{Criterio asintotico}

Consideriamo due serie $\sum a_n$ e $\sum b_n$. Supponiamo valga quanto
segue:
\[
\lim_{n \to \infty} \frac{a_n}{b_n} = L \in \ooint{0}{\infty}
\]
Se questa ipotesi \`e verificata, alla le due serie $\sum a_n$ e $\sum b_n$ 
o convergono entrambe o divergono entrambe.

\subsection{Criterio del rapporto}

Consideriamo una serie $\sum a_n$ tale per cui $a_n \ge 0$. Studiamo il 
limite per $n$ che tende all'infinito del rapporto fra il termine $n$-esimo
e il termine che lo precede. Vale questo:
\[
\lim_{n \to \infty} \frac{a_{n+1}}{a_n} =
\begin{cases}
L < 1 &\implies \text{ la serie converge} \\
L > 1 &\implies \text{ la serie diverge} \\
L = 1 &\implies \text{ il criterio non dice niente sulla serie}
\end{cases}
\]

\subsection{Criterio della radice}

Consideriamo una serie $\sum a_n$ tale per cui $a_n \ge 0$. Studiamo il 
limite per $n$ che tende all'infinito della radice $n$-esima del termine
$n$-esimo. Vale questo:
\[
\lim_{n \to \infty} \sqrt[n]{a_n} =
\begin{cases}
L < 1 &\implies \text{ la serie converge} \\
L > 1 &\implies \text{ la serie diverge} \\
L = 1 &\implies \text{ il criterio non dice niente sulla serie}
\end{cases}
\]

\subsection{Criterio di Leibniz}

Il criterio di Leibniz si applica a serie di segno alterno.
Sia $\sum a_n$ una serie, e supponiamo valga questo:
\begin{enumerate}
    \item la serie \`e a termini di segno alterno, ossia ogni termine
    ha segno opposto rispetto al successivo;
    \item $\abs{a_n} \ge \abs{a_{n+1}}$, ossia i termini della serie
    sono decrescenti (non strettamente);
    \item $\lim_{n \to \infty} a_n = 0$, ossia i termini della serie 
    tendono a 0.
\end{enumerate}
Se queste tre ipotesi sono verificate, allora la serie $\sum a_n$ converge
semplicemente.

\section{Serie armonica generalizzata}

La serie armonica generalizzata \`e una serie del tipo:
\[
\sum_{n = n_0}^{\infty} \frac{1}{n^p}
\]
Si distinguono due casi per la convergenza:
\[
\sum_{n = n_0}^{\infty} \frac{1}{n^p} =
\begin{cases}
\text{converge } &  \text{se } p > 1 \\
\text{diverge } & \text{se } p \le 1
\end{cases}
\]

\section{Serie di potenze}

Una serie di potenze \`e una serie del tipo:
\[
\sum_{n = n_0}^{\infty} \frac{\left[ f(x) \right]^{a \, n + b}}{a_n}
\]
Il centro di una serie di potenze \`e un valore $x_0$ tale per cui
$f(x_0) = 0$. Tipicamente le serie che si incontrano hanno la forma:
\[
\sum_{n = n_0}^{\infty} \frac{\left( x - x_0 \right)^{a \, n + b}}{a_n}
\]
In questo caso il centro \`e proprio $x_0$.

Di una serie di potenze, oltre al centro, vogliamo conoscere il raggio
di convergenza (per cui la serie converge assolutamente), e vogliamo poi
studiare se la serie converge semplicemente, converge assolutamente, o
diverge agli estremi dell'intervallo di convergenza.

Sia $R$ il raggio di convergenza di una serie di potenze, l'intervallo di 
convergenza \`e l'intervallo $\ooint{x_0 - R}{x_0 + R}$. Per trovare il 
raggio di convergenza di una serie di potenze, si risolve il seguente
limite:
\[
\lim_{n \to \infty} \abs{\frac{\left[f(x)\right]^{a \, (n+1) + b}}{\left[f(x)\right]^{{a \, n + b}}} \cdot \frac{a_n}{a_{n+1}}} = R(x)
\]
Il risultato sar\`a una funzione $R(x)$. Si impone $R(x) < 1$, per trovare
per quali valori di $x$ la serie converge assolutamente. 

Stiamo praticamente applicando il criterio del rapporto, non conosciamo la
$x$. Il criterio del rapporto ci dice che se il limite del rapporto fra un 
termine e il precedente \`e minore di 1, la serie converge. Per questo si 
impone $R(x) < 1$.

Si studiano infine i casi in cui $R(x) = 1$, e per ciascun caso si determina
se la serie converge semplicemente, converge assolutamente, o diverge.

\end{document}
